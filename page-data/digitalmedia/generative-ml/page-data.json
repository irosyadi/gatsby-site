{"componentChunkName":"component---src-templates-blog-post-js","path":"/digitalmedia/generative-ml/","result":{"data":{"site":{"siteMetadata":{"author":"Imron Rosyadi","comment":{"utterances":""},"sponsor":{"buyMeACoffeeId":"irosyadi"}}},"markdownRemark":{"excerpt":"Generative Machine Learning GAN GAN The dataset was web-scraped for an original 20k samples, then a custom MRCNN model was trained for image segmentation and cropping before being fed into the 128 DC‚Ä¶","html":"<h2 id=\"generative-machine-learning-gan\" style=\"position:relative;\"><a href=\"#generative-machine-learning-gan\" aria-label=\"generative machine learning gan permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Generative Machine Learning GAN</h2>\n<h3 id=\"gan\" style=\"position:relative;\"><a href=\"#gan\" aria-label=\"gan permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GAN</h3>\n<p>The dataset was web-scraped for an original 20k samples, then a custom MRCNN model was trained for image segmentation and cropping before being fed into the 128 DCGAN, trained on local hardware, 1660</p>\n<ul>\n<li><a href=\"https://github.com/w86763777/pytorch-gan-collections\">Collections of GANs : Pytorch implementation of unsupervised GANs.</a></li>\n</ul>\n<h3 id=\"generative-model-course\" style=\"position:relative;\"><a href=\"#generative-model-course\" aria-label=\"generative model course permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Generative Model Course</h3>\n<ul>\n<li><a href=\"https://courses.cs.washington.edu/courses/cse599i/20au/\">CSE 599</a></li>\n<li><a href=\"https://deepgenerativemodels.github.io/notes/index.html\">deepgenerativemodels</a></li>\n</ul>\n<h3 id=\"gan-course\" style=\"position:relative;\"><a href=\"#gan-course\" aria-label=\"gan course permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GAN Course</h3>\n<ul>\n<li><a href=\"https://www.deeplearning.ai/program/deep-learning-specialization/\">Deep Learning Specialization - DeepLearning.AI</a></li>\n<li><a href=\"https://college.berklee.edu/courses/mtec-345\">Machine Learning for Musicians - Berklee</a></li>\n<li><a href=\"https://www.kadenze.com/courses/machine-learning-for-music-information-retrieval/info\">MUSIC DATA MINING - a Music Information Retrieval (MIR) Online Course at Kadenze</a></li>\n<li><a href=\"https://www.kadenze.com/courses/foundations-of-arts-and-entertainment-technologies-i/info\">Arts and Entertainment Technology - Online Course - Kadenze</a></li>\n<li><a href=\"https://www.kadenze.com/courses/generative-art-and-computational-creativity/info\">Introduction to Generative Arts and Computational Creativity - an Online Course at Kadenze</a></li>\n<li><a href=\"https://instituteofcoding.org/courses/course/ual-apply-creative-machine-learning/\">UAL - Apply Creative Machine Learning - Institute of CodingInstitute of Coding</a></li>\n<li><a href=\"https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info\">Machine Learning for Musicians and Artists - an Online Machine Art Course at Kadenze</a></li>\n<li><a href=\"https://www.youtube.com/user/bustbright/playlists\">Artificial Images - YouTube</a></li>\n</ul>\n<h3 id=\"gan-projectpaper\" style=\"position:relative;\"><a href=\"#gan-projectpaper\" aria-label=\"gan projectpaper permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GAN Project/Paper</h3>\n<ul>\n<li><a href=\"https://github.com/NVlabs/stylegan3\">NVlabs/stylegan3: Official PyTorch implementation of StyleGAN3</a></li>\n<li><a href=\"https://www.nvidia.com/en-us/research/ai-demos/\">AI Demos - NVIDIA Research</a> GAN</li>\n<li><a href=\"https://yuval-alaluf.github.io/hyperstyle/\">HyperStyle</a> StyleGAN Inversion</li>\n<li><a href=\"https://old.pollinations.ai/\">Pollinations.AI</a></li>\n</ul>\n<h3 id=\"style-gan\" style=\"position:relative;\"><a href=\"#style-gan\" aria-label=\"style gan permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Style GAN</h3>\n<ul>\n<li><a href=\"https://www.gwern.net/Faces?ref=mlnews#examples\">Making Anime Faces With StyleGAN ¬∑ Gwern.net</a></li>\n<li><a href=\"https://github.com/cedricoeldorf/ConditionalStyleGAN\">cedricoeldorf/ConditionalStyleGAN: Conditional implementation for NVIDIA's StyleGAN architecture</a></li>\n<li>\n<p><a href=\"https://github.com/NVlabs/stylegan\">NVlabs/stylegan: StyleGAN - Official TensorFlow Implementation</a></p>\n<ul>\n<li><a href=\"https://old.reddit.com/r/computervision/comments/bfcnbj/p_stylegan_on_oxford_visual_geometry_group/\">P StyleGAN on Oxford Visual Geometry Group Flowers 102 Dataset üíêüåªüå∑ü•Äüå∫üåπüå∏üåº : computervision</a></li>\n<li><a href=\"https://www.robots.ox.ac.uk/~vgg/data/flowers/\">Visual Geometry Group - University of Oxford</a></li>\n</ul>\n</li>\n<li><a href=\"https://github.com/colinrsmall/ehm_faces\">colinrsmall/ehm_faces</a></li>\n<li><a href=\"https://nvlabs.github.io/stylegan2/versions.html\">StyleGAN versions</a></li>\n<li><a href=\"https://github.com/t04glovern/stylegan-pokemon\">t04glovern/stylegan-pokemon: Generating Pokemon cards using a mixture of StyleGAN and RNN to create beautiful &#x26; vibrant cards ready for battle!</a></li>\n</ul>\n<h3 id=\"paper\" style=\"position:relative;\"><a href=\"#paper\" aria-label=\"paper permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Paper</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2011.05552\">2011.05552End-to-End Chinese Landscape Painting Creation Using Generative Adversarial Networks</a></li>\n<li><a href=\"https://arxiv.org/abs/1812.04948\">1812.04948 A Style-Based Generator Architecture for Generative Adversarial Networks</a></li>\n<li><a href=\"https://nv-tlabs.github.io/editGAN/\">EditGAN</a></li>\n</ul>\n<h3 id=\"gan-image-superresolution\" style=\"position:relative;\"><a href=\"#gan-image-superresolution\" aria-label=\"gan image superresolution permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GAN Image Superresolution</h3>\n<ul>\n<li><a href=\"https://github.com/n00mkrad/cupscale\">Cupscale</a></li>\n<li><a href=\"https://github.com/xinntao/Real-ESRGAN\">Real-ESRGAN</a></li>\n<li><a href=\"https://filter.mot.omg.lol/\">hi, generate with your photo</a></li>\n</ul>\n<h3 id=\"gan-1\" style=\"position:relative;\"><a href=\"#gan-1\" aria-label=\"gan 1 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GAN</h3>\n<ul>\n<li><a href=\"https://www.gwern.net/Faces\">Making Anime Faces With StyleGAN ¬∑ Gwern.net</a></li>\n</ul>\n<h3 id=\"style-gan-1\" style=\"position:relative;\"><a href=\"#style-gan-1\" aria-label=\"style gan 1 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Style-GAN</h3>\n<ul>\n<li><a href=\"https://github.com/junyanz/CycleGAN\">junyanz/CycleGAN: Software that can generate photos from paintings, turn horses into zebras, perform style transfer, and more.</a></li>\n<li><a href=\"https://github.com/kaonashi-tyc/zi2zi\">kaonashi-tyc/zi2zi: Learning Chinese Character style with conditional GAN</a></li>\n<li><a href=\"https://github.com/rosinality/style-based-gan-pytorch\">rosinality/style-based-gan-pytorch: Implementation A Style-Based Generator Architecture for Generative Adversarial Networks in PyTorch</a></li>\n<li><a href=\"https://github.com/taki0112/StyleGAN-Tensorflow\">taki0112/StyleGAN-Tensorflow: Simple &#x26; Intuitive Tensorflow implementation of StyleGAN (CVPR 2019 Oral)</a></li>\n<li><a href=\"https://github.com/mtobeiyf/sketch-to-art\">mtobeiyf/sketch-to-art: üñº Create artwork from your casual sketch with GAN and style transfer</a></li>\n<li><a href=\"https://github.com/taki0112/UGATIT\">taki0112/UGATIT: Official Tensorflow implementation of U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (ICLR 2020)</a></li>\n<li><a href=\"https://github.com/zeka-io/selfie-to-anime\">zeka-io/selfie-to-anime</a></li>\n<li><a href=\"https://github.com/boistud/StyleArtGan\">boistud/StyleArtGan</a></li>\n<li><a href=\"https://github.com/heavenstobetsy/ArtGenerationwithStyleGan\">heavenstobetsy/ArtGenerationwithStyleGan: Fauvist art generation using StyleGAN</a></li>\n<li><a href=\"https://github.com/zhenxuan00/triple-gan\">zhenxuan00/triple-gan: See Triple-GAN-V2 in PyTorch: https://github.com/taufikxu/Triple-GAN</a></li>\n<li><a href=\"https://github.com/Mawiszus/TOAD-GAN\">Mawiszus/TOAD-GAN: Official repository for \"TOAD-GAN: Coherent Style Level Generation from a Single Example\" by Maren Awiszus, Frederik Schubert and Bodo Rosenhahn.</a></li>\n<li><a href=\"https://github.com/schrum2/GameGAN\">schrum2/GameGAN: Interactive GAN evolution of Mario and Zelda levels.</a></li>\n<li><a href=\"https://github.com/changebo/HCCG-CycleGAN\">changebo/HCCG-CycleGAN: Handwritten Chinese Characters Generation</a></li>\n</ul>\n<h3 id=\"research\" style=\"position:relative;\"><a href=\"#research\" aria-label=\"research permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Research</h3>\n<ul>\n<li><a href=\"https://www.sciencedirect.com/science/article/pii/S1474034621001336?via%3Dihub\">An enhanced 3D model and generative adversarial network for automated generation of horizontal building mask images and cloudless aerial photographs - ScienceDirect</a></li>\n</ul>\n<h3 id=\"research-1\" style=\"position:relative;\"><a href=\"#research-1\" aria-label=\"research 1 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Research</h3>\n<ul>\n<li><a href=\"https://realless.glitch.me/\">Realless</a> Generative webs with blinking eyes</li>\n</ul>\n<h3 id=\"gan-2\" style=\"position:relative;\"><a href=\"#gan-2\" aria-label=\"gan 2 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GAN</h3>\n<ul>\n<li><a href=\"https://huggingface.co/spaces/akhaliq/GFPGAN\">Gradio demo for GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior</a></li>\n</ul>\n<p>Stable Diffusion</p>\n<ul>\n<li><a href=\"https://jalammar.github.io/illustrated-stable-diffusion/\">The Illustrated Stable Diffusion‚ÄìJay Alammar‚ÄìVisualizing machine learning one concept at a time.</a></li>\n</ul>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","tableOfContents":"<ul>\n<li>\n<p><a href=\"/digitalmedia/generative-ml/#generative-machine-learning-gan\">Generative Machine Learning GAN</a></p>\n<ul>\n<li><a href=\"/digitalmedia/generative-ml/#gan\">GAN</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#generative-model-course\">Generative Model Course</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#gan-course\">GAN Course</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#gan-projectpaper\">GAN Project/Paper</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#style-gan\">Style GAN</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#paper\">Paper</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#gan-image-superresolution\">GAN Image Superresolution</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#gan-1\">GAN</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#style-gan-1\">Style-GAN</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#research\">Research</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#research-1\">Research</a></li>\n<li><a href=\"/digitalmedia/generative-ml/#gan-2\">GAN</a></li>\n</ul>\n</li>\n</ul>","frontmatter":{"date":"Sat, Jun 11, 2022","title":"Generative Machine Learning GAN","tags":["generative","machine","learning","GAN"]}}},"pageContext":{"slug":"/digitalmedia/generative-ml/","previous":{"fields":{"slug":"/academia/browser-research-tool/"},"frontmatter":{"title":"Awesome Browser Research Tools"}},"next":{"fields":{"slug":"/webapp/deepfake-not-exist/"},"frontmatter":{"title":"Deepfake - This Thing Does Not Exist"}}}},"staticQueryHashes":["1081905842","63159454"]}